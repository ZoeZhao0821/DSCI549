{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAd2m4ePGLCl"
   },
   "source": [
    "# A Notebook to Use Decision Tree Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlO9vzSQGVB2"
   },
   "source": [
    "This notebook shows how to train a decision tree to classify unseen instances.\n",
    "\n",
    "For those of you interested in understanding the code, it uses predefined functions from the [sklearn](http://scikit-learn.org) library of machine learning primitives and from the [graphviz](http://www.graphviz.org) library to generate visualizations. A few more details about the code:  \n",
    "\n",
    "The next two cells will download the data needed for this exercise and define useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_HVmfVTGWVk"
   },
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.download('https://raw.githubusercontent.com/khider/INF549/master/Assignment3_MachineLearning/Dataset/iris.csv')\n",
    "wget.download('https://raw.githubusercontent.com/khider/INF549/master/Assignment3_MachineLearning/Dataset/lenses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J_aMOusiGeof"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import graphviz\n",
    "\n",
    "def loadDataSet(dataset):\n",
    "    with open(dataset) as f:\n",
    "        data=f.readlines()\n",
    "        attributes=data[0].rstrip().split(',')[:-1]\n",
    "        instances=[entry.rstrip().split(',')[:-1] for entry in data[1:]]\n",
    "        dataArray=[]\n",
    "        for i in range(len(instances[0])):\n",
    "            dataArray.append([float(instance[i]) for instance in instances])\n",
    "        instances=np.array(dataArray).T\n",
    "        labels=[entry.rstrip().split(',')[-1] for entry in data[1:]]\n",
    "        return attributes,instances,labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cmap_data = plt.cm.Paired\n",
    "cmap_cv = plt.cm.coolwarm\n",
    "def plot_cv_indices(cv, X, y, ax, n_splits, lw=10):\n",
    "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
    "\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.array([np.nan] * len(X))\n",
    "        indices[tt] = 1\n",
    "        indices[tr] = 0\n",
    "        # Visualize the results\n",
    "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
    "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
    "                   vmin=-.2, vmax=1.2)\n",
    "\n",
    "    # Plot the data classes and groups at the end\n",
    "    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
    "              c=y, marker='_', lw=lw, cmap=cmap_data)\n",
    "    \n",
    "    # Formatting\n",
    "    yticklabels = list(range(n_splits)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_splits+1) + .5, yticklabels=yticklabels,\n",
    "           xlabel='Sample index', ylabel=\"CV iteration\",\n",
    "           ylim=[n_splits+2.2, -.2], xlim=[0, len(X)])\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_bpkY00Gl6x"
   },
   "source": [
    "## Training: Building a Decision Tree Classifier ##\n",
    "\n",
    "The cell below asks for a dataset. It trains a decision tree classifier. \n",
    "\n",
    "We provide two classification datasets that could be applied to the decision tree algorithms. \n",
    "* \"iris.csv\" has four attributes with continuous values describing three different iris species.\n",
    "* \"lenses.csv\" contains four attributes with discrete values and three classes.\n",
    "\n",
    "Before training your classifier, run the cell below to take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaGF8KJRGhQY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset=input('Please Enter Your Dataset:')\n",
    "df=pd.read_csv(dataset)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hOcX98MkGsGw"
   },
   "source": [
    "The following cell gives the number of instances in all distinct classes.\n",
    "\n",
    "Label Name is the name of the last column in the dataframe. Note: This is case sensitive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3xOFuckgGsfX"
   },
   "outputs": [],
   "source": [
    "label_name = input('Enter The Label Name:')\n",
    "print(df[label_name].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95RPiR-CG21a"
   },
   "source": [
    "Before we run the following cell, let's learn an important concept called feature encoding. Many classifiers only take numerical data and some datasets have features that are not numerical. For example, a feature can be the state that a person lives in. Those are called [categorical features](https://en.wikipedia.org/wiki/Categorical_variable). In that case,we need to encode categorical features into discrete values. This process is called feature encoding\n",
    "\n",
    "In our notebook, if your dataset contains categorical features, you will see the code rules in the cell below. In the next section, when you are prompted to input test set for prediction, the algorithm will automatically encode the relevant categorical features according to the code rules showned below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cCsuSUodG3Lh"
   },
   "outputs": [],
   "source": [
    "attributes,instances,labels=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(instances,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kN9x8TJUG9HI"
   },
   "source": [
    "## Visualizing a Decision Tree##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6flkKTxaHCGk"
   },
   "source": [
    "The following cell will generate a visualization of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eOHcqZCuG_fP"
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None,max_depth=5,\\\n",
    "feature_names=attributes,class_names=clf.classes_,label='all',\\\n",
    "filled=True,special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TU67ZUzHHKi6"
   },
   "source": [
    "## Evaluating the Accuracy of a Decision Tree Classifier##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFts6pt-HTVX"
   },
   "source": [
    "The following cell will run cross-validation to evaluate your decision tree classifier.  It will ask you for your test data, and the number of folds that you want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7chxB_7HWRq"
   },
   "source": [
    "K Fold Cross-Validation is used to estimate prediction error. The dataset is randomly divided into K folds. The first fold acts as the validation set while the method is fit on remaining K-1 folds. Mean Squared Error is calculated on the observations from the held-out fold. The process is repeated K times, taking a different part each time. \n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=12dKPXQ3m_-283DfvC2Tgy0rOU5gnY277)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5gXsFQcHbz8"
   },
   "source": [
    "You can visualize the testing and trainging in KFold cross-validation in the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3ElIQJXHfDR"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "fig, ax = plt.subplots()\n",
    "n_splits=int(input(\"Please Enter the Number of Folds:\"))\n",
    "cv = KFold(n_splits)\n",
    "if label_name == \"type of iris plant\":\n",
    "  df[class_label]=df[class_label].map({'Iris-setosa':'0','Iris-versicolor':'1', 'Iris-virginica':'2'})\n",
    "  X = np.array(df.iloc[:, 0:4])\n",
    "  y = np.array(df.iloc[:, 4:5])\n",
    "if label_name == \"type of lens\":\n",
    "  sorted_df = df.sort_values('type of lens')\n",
    "  X = np.array(sorted_df.iloc[:, 0:4])\n",
    "  y = np.array(sorted_df.iloc[:, 4:5])\n",
    "y = y.astype(np.int)\n",
    "plot_cv_indices(cv, X, y, ax, n_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIp1zl00HlKC"
   },
   "source": [
    "The following cell will output the accuracy score for each fold and the accuracy estimate of the model under 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S_DqDYgRHzB_"
   },
   "outputs": [],
   "source": [
    "attributes,instances,labels=loadDataSet(dataset)\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(instances,labels)\n",
    "scores = cross_val_score(clf, instances, labels, cv=n_splits)\n",
    "print(\"Scores:\")\n",
    "[print(score) for score in scores]\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0Yn2TOqHHKz"
   },
   "source": [
    "## Prediction: Classifying New Instances Using a Decision Tree Classifier##\n",
    "\n",
    "The cell below classifies new instances with the decision tree you created.\n",
    "\n",
    "When you are prompted to input a test set, please create an example of an instance that looks like the instances in the training set.  \n",
    "\n",
    "Each feature value is separated with a comma, and should have the same length as the instances in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ku7-hcl_HHfi"
   },
   "outputs": [],
   "source": [
    "testset=input('Please Enter Your Prediction Set:')\n",
    "testset=testset.strip().split(\",\")\n",
    "temp=[]\n",
    "for i in range(len(testset)):\n",
    "        temp.append(float(testset[i]))\n",
    "testset=np.array(temp).reshape((1,len(temp)))\n",
    "predictions=clf.predict(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Notebook1_Decision_Tree_Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
